{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import jit, cuda, vectorize, float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 First testing numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cov = np.random.randn(100).reshape((10, 10))\n",
    "vec = np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.2643887216958039"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(np.dot(vec, cov), vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.44 s ± 52.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "@numba.jit(nopython=True)\n",
    "def sum2d(M):\n",
    "    summation = 0.\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            summation += np.dot(np.dot(vec, cov), vec)\n",
    "    return summation\n",
    "%timeit sum2d(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.1 s ± 998 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "def sum2d(M):\n",
    "    summation = 0.\n",
    "    for i in range(M):\n",
    "        for j in range(M):\n",
    "            summation += np.dot(np.dot(vec, cov), vec)\n",
    "    return summation\n",
    "%timeit sum2d(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Need NVIDIA gpu to run CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below runs on i7-7700K and NVIDIA TITAN Xp, a powerful GPU over a thousand dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Managed Device 0>\n"
     ]
    }
   ],
   "source": [
    "print(cuda.gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Vectorize_add\n",
    "### It seems that target=cpu is better than target=cuda. Unexpected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize([float64(float64,float64)], target='cuda')\n",
    "def add_vec_cuda(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@vectorize([float64(float64, float64)], target='cpu')\n",
    "def add_vec_cpu(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def add_jit(X, Y):\n",
    "    size = len(X)\n",
    "    Z = []\n",
    "    for i in range(size):\n",
    "        Z.append(X[i] + Y[i])\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 ms ± 272 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "27.2 µs ± 142 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n",
      "2.32 ms ± 115 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(100000, dtype=np.float64)\n",
    "%timeit add_vec_cuda(a, a)\n",
    "%timeit add_vec_cpu(a, a)\n",
    "%timeit add_jit(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.62 ms ± 25.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "644 µs ± 9.42 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "28.4 ms ± 183 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(1000000, dtype=np.float64)\n",
    "%timeit add_vec_cuda(a, a)\n",
    "%timeit add_vec_cpu(a, a)\n",
    "%timeit add_jit(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 ms ± 985 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "25.4 ms ± 228 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "344 ms ± 658 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(10000000, dtype=np.float64)\n",
    "%timeit add_vec_cuda(a, a)\n",
    "%timeit add_vec_cpu(a, a)\n",
    "%timeit add_jit(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549 ms ± 15.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "256 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "3.39 s ± 9.54 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(100000000, dtype=np.float64)\n",
    "%timeit add_vec_cuda(a, a)\n",
    "%timeit add_vec_cpu(a, a)\n",
    "%timeit add_jit(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Vectorize_sincos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "x = np.arange(n, dtype=np.float64)\n",
    "y = np.arange(n, dtype=np.float64)\n",
    "\n",
    "np_ans = np.sin(x) * np.cos(y)\n",
    "nb_cpu_ans = cpu_sincos(x, y)\n",
    "nb_gpu_ans = gpu_sincos(x, y)\n",
    "\n",
    "print(\"CPU vectorize:\", np.allclose(nb_cpu_ans, np_ans))\n",
    "print(\"CPU vectorize:\", np.allclose(nb_gpu_ans, np_ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.45464871, -0.37840125, ..., -0.20931808,\n",
       "        0.49999812, -0.2068272 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.45464871, -0.37840125, ..., -0.20931808,\n",
       "        0.49999812, -0.2068272 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Kernal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1953125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., ..., 2., 2., 2.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def my_kernal(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size: # check array boundaries\n",
    "        io_array[pos] *= 2  # do the computation\n",
    "        \n",
    "data = np.ones(500000000)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0]/threadsperblock)\n",
    "print(blockspergrid)\n",
    "my_kernal[blockspergrid, threadsperblock](data)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    row, col = cuda.grid(2)\n",
    "    #print(row, col)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def host_code():\n",
    "    A = np.full((24, 12), 3, np.float)\n",
    "    B = np.full((12, 6), 4, np.float)\n",
    "    #copy the arrays to the device\n",
    "    A_global_mem = cuda.to_device(A)\n",
    "    B_global_mem = cuda.to_device(B)\n",
    "    \n",
    "    #allocate memory on the device for the result\n",
    "    C_global_mem = cuda.device_array((24, 6))\n",
    "    \n",
    "    # configure the blocks\n",
    "    threadsperblock = (16, 16)\n",
    "    blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "    blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "    print(blockspergrid)\n",
    "    \n",
    "    #start the kernal\n",
    "    matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "    \n",
    "    C = C_global_mem.copy_to_host()\n",
    "    print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "[[144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]\n",
      " [144. 144. 144. 144. 144. 144.]]\n"
     ]
    }
   ],
   "source": [
    "host_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the computation will be done on blocks of TPB*TPB elements\n",
    "TPB = 16\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    '''matrix multiplication C = A*B\n",
    "       each thread computes one element of the result matrix C\n",
    "    '''\n",
    "    # define an array in the shared memory\n",
    "    # the size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=float64)\n",
    "    \n",
    "    x, y = cuda.grid(2)\n",
    "    \n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    \n",
    "    if x >= C.shape[0] or y >= C.shape[1]:\n",
    "        return\n",
    "    \n",
    "    # each thread computes one element in the result matrix\n",
    "    # the dot product is chunked into dot products of TPB-long vectors\n",
    "    tmp = 0\n",
    "    for i in range(int(A.shape[1]/TPB)):\n",
    "        # preload data into shared memory\n",
    "        sA[tx, ty] = A[x, ty + i*TPB]\n",
    "        sB[tx, ty] = B[tx + i*TPB, y]\n",
    "        \n",
    "        # wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[tx, j]*sB[j, ty]\n",
    "        \n",
    "        # wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    \n",
    "    C[x, y] = tmp\n",
    "    \n",
    "    \n",
    "def host_code():\n",
    "    # initialize the data arrays\n",
    "    A = np.full((TPB*2, TPB*3), 3, np.float64)  # [32 x 48] matrix containing all 3's\n",
    "    B = np.full((TPB*3, TPB*1), 4, np.float64)  # [48 x 16] matrix containing all 4's\n",
    "    \n",
    "    A_global_mem = cuda.to_device(A)\n",
    "    B_global_mem = cuda.to_device(B)\n",
    "    C_global_mem = cuda.device_array((TPB*2, TPB*1))  # [32 x 16] matrix result\n",
    "    \n",
    "    # configure the blocks\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[1]))\n",
    "    blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[0]))\n",
    "    blockspergrid   = (blockspergrid_x, blockspergrid_y)\n",
    "    \n",
    "    # start the kernal\n",
    "    fast_matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "    \n",
    "    res = C_global_mem.copy_to_host()\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]\n",
      " [576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576. 576.\n",
      "  576. 576.]]\n"
     ]
    }
   ],
   "source": [
    "host_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_gpu = numba.cuda.get_current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: b'TITAN Xp'\n",
      "Compute capability: 6.1\n",
      "Number of streaming multiprocessor: 30\n",
      "Warp size: 32\n"
     ]
    }
   ],
   "source": [
    "print('Running on GPU:', my_gpu.name)\n",
    "cc = my_gpu.compute_capability\n",
    "print('Compute capability:', '%d.%d' % cc)\n",
    "majorcc = cc[0]\n",
    "print('Number of streaming multiprocessor:', my_gpu.MULTIPROCESSOR_COUNT)\n",
    "print('Warp size:', my_gpu.WARP_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: b'GeForce GTX 1070'\n"
     ]
    }
   ],
   "source": [
    "my_gpu = numba.cuda.get_current_device()\n",
    "print('Running on GPU:', my_gpu.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.reduce\n",
    "def sum_reduce(a, b):\n",
    "    return a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.99243252e-201, 8.12433843e-202, 2.06057770e-201, 2.11568071e-201,\n",
       "       1.55664446e-201, 9.66401485e-201, 9.20515738e-201, 7.72448510e-203,\n",
       "       1.87608744e-201, 2.20562155e-201, 2.35507333e-201, 9.38715782e-201,\n",
       "       1.62888949e-201, 7.07204916e-201, 8.22117692e-201, 6.87778031e-202,\n",
       "       1.99544743e-201, 1.48369256e-202, 7.69158783e-201, 5.78088619e-201,\n",
       "       6.05913133e-201, 2.70405787e-202, 6.44419141e-201, 5.35446603e-201,\n",
       "       7.14491557e-201, 5.63080554e-201, 4.03097434e-201, 2.63886607e-201,\n",
       "       7.14761070e-201, 9.55237499e-201, 5.74633931e-202, 1.29778028e-201,\n",
       "       7.45739347e-201, 1.07503370e-201, 7.48496233e-201, 2.61831226e-201,\n",
       "       6.42456894e-201, 5.61207085e-201, 1.51400501e-201, 4.74511242e-201,\n",
       "       2.04818946e-201, 2.78280515e-201, 7.67783910e-201, 6.14282350e-201,\n",
       "       7.86927547e-201, 1.16825348e-201, 9.27018507e-201, 6.33715915e-201,\n",
       "       4.19510649e-201, 9.30240243e-201, 5.99176830e-201, 9.59921284e-201,\n",
       "       6.17749862e-202, 9.06185082e-201, 9.17202931e-201, 7.20656061e-201,\n",
       "       3.00751672e-201, 8.75428297e-203, 5.46288669e-201, 7.19411014e-201,\n",
       "       9.10723025e-201, 9.43172656e-201, 2.52069537e-201, 9.95107298e-201,\n",
       "       3.04401907e-201, 5.29611870e-201, 5.56828619e-202, 8.28179293e-201,\n",
       "       6.15291355e-201, 5.81551358e-201, 6.65250488e-201, 7.83394398e-201,\n",
       "       3.85475799e-201, 6.73312520e-201, 9.03918355e-201, 7.35482582e-201,\n",
       "       3.60299331e-202, 2.04953685e-201, 4.45739437e-202, 9.85226703e-201,\n",
       "       6.91163762e-201, 2.20300965e-201, 5.31140334e-201, 8.90933939e-201,\n",
       "       6.85636822e-201, 6.67160924e-202, 5.03124784e-201, 4.86648597e-201,\n",
       "       4.44788517e-202, 3.43107807e-202, 1.07349195e-201, 8.48831071e-201,\n",
       "       2.65599475e-201, 2.59230400e-201, 5.65943248e-201, 8.64896270e-201,\n",
       "       6.07093365e-201, 5.87540820e-201, 2.74082828e-201, 5.55435059e-201,\n",
       "       1.74520659e-201, 3.49151603e-201, 7.60239755e-201, 3.25630280e-201,\n",
       "       8.91500842e-201, 8.33753226e-202, 5.60076751e-202, 8.92317424e-201,\n",
       "       8.00577720e-201, 8.33095091e-201, 3.07883167e-201, 1.66984669e-201,\n",
       "       2.19859437e-202, 7.27936779e-202, 1.04456156e-201, 2.01077392e-201,\n",
       "       5.49915839e-201, 6.33180320e-201, 6.12836130e-201, 9.42441435e-201,\n",
       "       6.17142292e-202, 5.18582635e-201, 7.01416134e-201, 1.74751692e-201,\n",
       "       3.82380372e-201, 6.56504767e-201, 4.04097209e-201, 9.57818659e-201,\n",
       "       3.98590607e-202, 1.72876109e-201, 5.49245868e-201, 4.71909669e-202,\n",
       "       7.39895972e-201, 9.14017574e-201, 2.63118040e-201, 1.53360528e-201,\n",
       "       9.29757777e-201, 5.19650575e-201, 3.33207715e-201, 9.66391811e-201,\n",
       "       9.66784716e-201, 9.30575181e-201, 8.73248720e-201, 6.95346964e-201,\n",
       "       9.15918270e-201, 1.77198520e-202, 8.85417295e-202, 1.05172127e-201,\n",
       "       9.41467077e-201, 8.08710276e-201, 5.87267107e-201, 6.10514050e-201,\n",
       "       6.43761879e-201, 9.63143698e-201, 3.60505357e-201, 3.33383497e-201,\n",
       "       7.85462333e-201, 5.47379726e-202, 5.21733259e-201, 5.89144994e-201,\n",
       "       5.57096864e-201, 8.06422940e-201, 7.37605863e-201, 1.51339566e-201,\n",
       "       4.10801535e-201, 9.57790526e-201, 7.67346522e-201, 1.32588864e-202,\n",
       "       6.68075032e-201, 8.46254357e-201, 3.28118497e-201, 7.22121236e-201,\n",
       "       5.05642781e-201, 1.00346987e-201, 8.20866821e-201, 7.03349957e-201,\n",
       "       1.62958489e-201, 3.64472886e-201, 9.37982555e-201, 3.72537603e-202,\n",
       "       9.98523169e-201, 9.33976078e-202, 8.32152957e-201, 7.79703772e-201,\n",
       "       6.21311747e-201, 7.85764440e-201, 9.09372644e-201, 2.18956756e-201,\n",
       "       1.73955001e-201, 8.62221142e-201, 9.11199282e-201, 5.88220655e-201,\n",
       "       8.37322017e-201, 4.37540592e-201, 8.48641719e-201, 9.98178975e-201,\n",
       "       5.43907240e-201, 7.24628138e-201, 5.77062239e-201, 4.56658160e-201,\n",
       "       5.94391931e-201, 6.93532901e-201, 6.04610934e-201, 4.52337299e-201,\n",
       "       4.28790629e-201, 6.23510148e-201, 8.92283301e-201, 1.83282315e-201,\n",
       "       4.24400750e-201, 7.94455304e-201, 6.48320022e-201, 9.25603444e-201,\n",
       "       2.44934518e-201, 7.66003266e-201, 7.47851186e-201, 7.53490476e-201,\n",
       "       9.58571345e-202, 8.13636528e-201, 5.37406409e-201, 4.85731348e-201,\n",
       "       9.03878106e-201, 8.79878096e-201, 3.54079116e-201, 4.93211974e-201,\n",
       "       4.46538177e-201, 7.53298696e-201, 9.19138863e-202, 7.38056385e-201,\n",
       "       7.66319324e-201, 6.38209297e-202, 5.89751228e-201, 5.69032132e-201,\n",
       "       7.73976109e-201, 5.09301120e-201, 9.18556967e-201, 3.92585013e-201,\n",
       "       4.16710339e-201, 6.22034677e-201, 8.94214509e-201, 6.73037242e-201,\n",
       "       8.71944742e-201, 3.59604991e-201, 4.78937665e-202, 1.50764513e-201,\n",
       "       8.59144052e-201, 2.09603505e-201, 3.08622406e-201, 5.70556210e-201,\n",
       "       6.68472671e-201, 3.38253103e-201, 1.89636007e-201, 5.59251135e-201,\n",
       "       2.98986596e-201, 5.67934418e-201, 3.11206272e-202, 3.83851438e-201,\n",
       "       1.48512926e-201, 2.84472686e-201, 6.51839439e-201, 3.61699294e-201,\n",
       "       2.67142767e-201, 6.52374384e-201, 2.77406590e-201, 7.37471776e-201,\n",
       "       2.98525397e-201, 1.84899068e-201, 8.57657279e-201, 7.25385034e-201,\n",
       "       6.58896198e-201, 9.24817259e-201, 6.45243855e-201, 5.34965741e-201,\n",
       "       6.77046196e-202, 1.18413743e-201, 1.56736446e-201, 7.12580916e-201,\n",
       "       9.00680179e-202, 9.74852925e-201, 2.05965623e-201, 1.64484186e-201,\n",
       "       3.05825839e-201, 6.36301071e-201, 6.59330039e-201, 9.74394798e-201,\n",
       "       6.19600877e-201, 3.89966072e-201, 5.51774802e-201, 3.18758830e-201,\n",
       "       3.91038467e-201, 7.84027671e-201, 1.42635884e-201, 1.11970484e-201,\n",
       "       6.08803692e-201, 7.17845156e-201, 5.68254348e-201, 1.28395810e-201,\n",
       "       2.86048696e-201, 7.64835513e-201, 3.33424477e-201, 2.71999620e-201,\n",
       "       2.51955185e-201, 4.67670124e-201, 3.47206223e-201, 9.11277764e-201,\n",
       "       6.26076953e-201, 4.18303460e-201, 8.59717278e-201, 6.37811808e-201,\n",
       "       8.75226159e-201, 7.89503204e-201, 1.26579082e-201, 9.07423925e-201,\n",
       "       9.56667110e-201, 7.00764042e-201, 9.46371508e-201, 1.22833562e-201,\n",
       "       3.01946177e-201, 9.09010509e-201, 5.99935736e-201, 7.02985996e-201,\n",
       "       5.29013134e-201, 3.72200455e-201, 7.82712298e-201, 5.93535579e-201,\n",
       "       1.77586349e-201, 4.62140468e-201, 4.94364808e-202, 7.77297820e-202,\n",
       "       3.69376630e-201, 8.28818912e-201, 1.91830411e-201, 6.24459335e-201,\n",
       "       4.31728345e-201, 5.88852662e-201, 4.05942233e-201, 3.62012770e-201,\n",
       "       4.21831064e-201, 7.77061491e-201, 4.97353865e-202, 3.62143666e-201,\n",
       "       7.52204784e-202, 5.34263120e-201, 3.64015132e-201, 1.66774505e-201,\n",
       "       9.96863375e-201, 9.16342861e-202, 6.97070543e-201, 4.74440485e-201,\n",
       "       8.45459065e-201, 9.94515851e-201, 7.49805551e-201, 2.71410702e-201,\n",
       "       7.07193020e-201, 6.37286577e-201, 5.46034412e-201, 8.06015230e-201,\n",
       "       7.67426318e-201, 5.32379984e-201, 5.55038401e-201, 1.53234462e-201,\n",
       "       3.93493330e-201, 8.29454086e-201, 6.97673604e-201, 9.18213913e-201,\n",
       "       3.17302482e-201, 4.32983791e-202, 2.82281166e-201, 3.23348078e-201,\n",
       "       5.53826163e-201, 1.48874745e-202, 7.68014072e-201, 6.25068642e-201,\n",
       "       9.58168372e-202, 3.65181025e-201, 6.17804611e-201, 5.77612306e-201,\n",
       "       2.69187396e-202, 1.67603414e-202, 5.66878058e-201, 3.62182520e-201,\n",
       "       8.35145951e-201, 4.99944303e-201, 8.96277026e-201, 2.24650613e-201,\n",
       "       6.30255270e-201, 6.59114434e-201, 3.69811395e-201, 2.99940522e-201,\n",
       "       4.70898856e-201, 4.13111203e-201, 8.27594637e-201, 6.42447924e-201,\n",
       "       2.06035222e-201, 3.04490166e-201, 3.51597064e-201, 3.46313309e-202,\n",
       "       9.04014320e-201, 3.77991963e-201, 5.99796948e-201, 7.87510919e-201,\n",
       "       7.93396871e-201, 5.61885683e-201, 3.41760148e-201, 5.14644377e-202,\n",
       "       2.75617626e-201, 3.59853821e-201, 6.94078356e-201, 1.45716003e-201,\n",
       "       8.75523861e-201, 9.96377900e-201, 1.18157939e-201, 4.00745983e-201,\n",
       "       4.73060433e-201, 1.86052676e-201, 4.93791661e-201, 3.01201259e-201,\n",
       "       9.23424793e-201, 1.52288706e-201, 5.44248305e-201, 4.29848411e-201,\n",
       "       8.10224246e-201, 5.63352177e-201, 3.11642592e-202, 4.55579221e-201,\n",
       "       3.51696438e-201, 1.18389113e-201, 2.02019098e-202, 5.98948907e-201,\n",
       "       7.21485922e-201, 6.84981932e-201, 3.21199737e-202, 6.44615049e-201,\n",
       "       9.83617436e-201, 8.51721701e-201, 9.54805184e-201, 5.69693067e-202,\n",
       "       7.40684133e-201, 6.86010079e-201, 1.98778238e-201, 7.52359900e-201,\n",
       "       2.33898916e-201, 2.18243763e-201, 9.36011123e-201, 4.47063514e-201,\n",
       "       8.98309243e-201, 4.91016956e-201, 5.91396765e-201, 3.20275263e-201,\n",
       "       6.92265547e-201, 8.20965619e-202, 4.24529795e-201, 3.58723645e-201,\n",
       "       1.39891805e-201, 8.74918963e-202, 2.96958323e-201, 8.73069520e-201,\n",
       "       8.05924506e-201, 2.14632077e-202, 7.60983596e-201, 6.13854820e-201,\n",
       "       4.19605338e-202, 3.56968729e-201, 9.84990945e-201, 1.83238418e-201,\n",
       "       8.80425066e-201, 4.79869847e-201, 1.07893806e-201, 6.85191017e-201,\n",
       "       5.68277009e-201, 6.21154497e-201, 8.04295944e-201, 5.56117565e-201,\n",
       "       5.75415843e-201, 7.71823689e-201, 3.21068692e-201, 5.66064502e-201,\n",
       "       4.04201094e-201, 7.81626240e-201, 4.83198075e-201, 5.74321015e-201,\n",
       "       7.21373943e-201, 1.38155026e-201, 2.84406493e-201, 6.07101915e-201,\n",
       "       2.60468962e-201, 5.11130264e-201, 1.23632355e-202, 2.80246692e-201,\n",
       "       7.17085682e-201, 5.49959816e-201, 8.92328053e-201, 2.64498936e-203,\n",
       "       8.38336789e-201, 6.74748570e-201, 7.31459711e-201, 4.32926536e-201,\n",
       "       3.52542751e-201, 4.40742388e-201, 7.68908666e-201, 6.50467205e-201,\n",
       "       6.76700016e-201, 3.11851139e-201, 3.25808376e-201, 4.05755163e-201,\n",
       "       9.03080169e-201, 5.28817086e-201, 9.31129587e-203, 5.34886543e-201,\n",
       "       1.25105724e-201, 9.10985201e-201, 8.44785128e-201, 2.76179067e-201,\n",
       "       1.34620275e-201, 3.94320943e-201, 7.30559306e-201, 6.79104252e-201,\n",
       "       6.56621444e-202, 6.99512609e-201, 7.35821740e-202, 6.66461121e-201,\n",
       "       1.88207525e-201, 1.63825305e-201, 8.13771950e-201, 7.23046087e-201,\n",
       "       3.54390775e-202, 2.84933648e-202, 4.84091340e-201, 3.37334078e-201,\n",
       "       9.51921786e-201, 6.32728458e-201, 1.39910847e-201, 3.52294937e-202,\n",
       "       5.55001111e-201, 1.62972954e-201, 9.71299708e-201, 9.26409492e-201,\n",
       "       8.04858702e-201, 5.26491227e-201, 4.06159716e-201, 9.10570891e-201,\n",
       "       2.56535802e-201, 5.68019528e-201, 7.19427478e-201, 8.06516343e-201,\n",
       "       7.19834468e-201, 6.75141502e-201, 3.07529236e-201, 8.27181142e-201,\n",
       "       8.91433111e-201, 9.16134100e-201, 2.03584624e-201, 3.76593658e-201,\n",
       "       5.51738010e-201, 6.54284149e-201, 2.22711826e-201, 7.33504919e-201,\n",
       "       9.35369865e-202, 3.17199782e-201, 7.74975457e-201, 1.59231010e-201,\n",
       "       2.02244586e-201, 8.72456235e-201, 4.14108841e-201, 8.50491545e-201,\n",
       "       3.86186566e-201, 5.47840011e-201, 6.40470342e-201, 7.13237469e-201,\n",
       "       7.78090542e-201, 4.25787550e-201, 2.49039914e-201, 1.97022035e-201,\n",
       "       3.14806694e-201, 5.43121992e-201, 4.70848173e-201, 3.05067863e-201,\n",
       "       2.63905904e-201, 4.00863669e-201, 2.64870758e-201, 9.89833332e-201,\n",
       "       5.88817649e-201, 2.03324775e-201, 1.84113885e-201, 2.93829886e-201,\n",
       "       5.24127162e-201, 7.15073960e-201, 9.28709151e-201, 3.88072412e-201,\n",
       "       6.93570984e-201, 8.93062083e-202, 9.94658935e-201, 8.02070335e-201,\n",
       "       5.65426850e-201, 3.70352243e-201, 4.35370573e-202, 6.63152912e-202,\n",
       "       2.19965813e-201, 3.70832647e-201, 9.35449015e-202, 7.15857442e-201,\n",
       "       9.95644600e-201, 5.34959108e-201, 8.82465800e-201, 2.41990598e-201,\n",
       "       8.62959891e-203, 3.02798380e-202, 9.81410977e-201, 9.05866147e-201,\n",
       "       4.62861020e-201, 6.55384655e-202, 9.32289637e-201, 5.79658113e-201,\n",
       "       2.20253708e-201, 4.26666091e-201, 8.29023920e-201, 9.11473961e-201,\n",
       "       8.84481885e-201, 7.36732656e-201, 5.59340877e-202, 5.09054813e-201,\n",
       "       9.26264934e-201, 9.48862156e-201, 7.22520523e-201, 1.70570074e-201,\n",
       "       3.13198917e-201, 2.44083037e-201, 7.30994297e-202, 2.05471207e-201,\n",
       "       7.60507939e-201, 1.84992854e-201, 2.40303727e-201, 5.43515869e-201,\n",
       "       7.56137265e-201, 6.98121085e-201, 2.07310001e-201, 6.99997477e-201,\n",
       "       5.17383963e-201, 5.30982583e-202, 4.16505650e-201, 2.05832817e-201,\n",
       "       9.79618380e-201, 8.63345011e-201, 5.49740731e-201, 4.90844318e-201,\n",
       "       3.98207721e-201, 7.20869868e-202, 2.55646154e-201, 8.86245263e-201,\n",
       "       5.85698210e-201, 9.07586862e-201, 9.42129918e-201, 3.89273414e-202,\n",
       "       3.77055983e-201, 3.91243979e-201, 4.66295230e-201, 6.39639892e-201,\n",
       "       5.00198159e-201, 6.18234999e-201, 1.75156497e-201, 9.08577846e-201,\n",
       "       7.05916322e-201, 3.34268838e-201, 3.41642689e-201, 3.17666758e-201,\n",
       "       6.41290495e-201, 8.46090248e-201, 1.58320547e-201, 9.49565317e-201,\n",
       "       1.49743284e-201, 5.33804587e-201, 1.13090782e-201, 6.61041447e-201,\n",
       "       2.06349835e-201, 6.36831743e-201, 7.73448783e-201, 1.86548674e-201,\n",
       "       4.61389907e-201, 7.93375644e-201, 5.87418700e-202, 2.62305215e-201,\n",
       "       3.22783284e-201, 7.76141131e-201, 9.65418019e-201, 5.14599924e-201,\n",
       "       8.42869723e-201, 3.50506716e-201, 5.38364924e-201, 2.74509328e-201,\n",
       "       3.39683236e-201, 4.79178051e-201, 3.19332239e-202, 5.66900544e-201,\n",
       "       9.59043375e-201, 1.92708698e-201, 4.47627701e-201, 9.49843089e-201,\n",
       "       4.47430061e-201, 1.64960904e-201, 7.12742723e-201, 3.11447907e-201,\n",
       "       7.76424950e-201, 7.60222591e-201, 7.38928938e-201, 7.76403314e-202,\n",
       "       9.26774869e-202, 9.03463029e-202, 9.28447206e-201, 3.91807407e-201,\n",
       "       7.27756621e-201, 3.14760296e-201, 5.66022697e-202, 8.56201917e-201,\n",
       "       4.72880742e-201, 4.54603283e-201, 8.27402465e-201, 9.52518093e-201,\n",
       "       8.13328685e-201, 5.88919599e-201, 8.61123445e-201, 2.61650289e-201,\n",
       "       1.74578014e-201, 4.99138684e-201, 6.21548521e-201, 4.03911387e-201,\n",
       "       6.34230722e-201, 9.00874365e-201, 8.49909110e-201, 3.45772765e-201,\n",
       "       3.24412211e-201, 2.72114173e-201, 8.42877462e-201, 4.78055656e-201,\n",
       "       7.31306089e-201, 8.01667785e-201, 6.91030930e-201, 2.75807209e-202,\n",
       "       1.88053238e-201, 2.43886179e-201, 6.85019927e-201, 7.27116235e-201,\n",
       "       6.46845484e-202, 1.18346136e-201, 2.93202432e-201, 3.07420156e-201,\n",
       "       2.26074859e-201, 6.35389560e-201, 6.29770232e-201, 9.98765429e-201,\n",
       "       3.10129064e-201, 8.55294483e-202, 6.95257723e-201, 5.20153003e-201,\n",
       "       1.54143166e-201, 8.42099264e-201, 1.51732686e-201, 4.01098918e-201,\n",
       "       7.30303029e-201, 3.51586003e-201, 2.89031279e-201, 5.54401201e-201,\n",
       "       8.05462897e-201, 5.86369929e-201, 3.69892525e-201, 8.57341038e-201,\n",
       "       4.15195715e-201, 8.64360483e-201, 4.28723699e-201, 5.06795253e-201,\n",
       "       9.67918357e-201, 2.22754291e-201, 9.41823652e-202, 5.12808025e-201,\n",
       "       4.83373380e-201, 7.13052068e-201, 8.66487300e-201, 5.89058435e-202,\n",
       "       1.94416493e-201, 7.26751781e-201, 6.61127818e-201, 5.91944657e-201,\n",
       "       6.47972322e-202, 7.41898157e-201, 4.43311183e-201, 3.34358020e-201,\n",
       "       9.91264969e-201, 7.27114354e-201, 9.74059300e-201, 2.14325791e-201,\n",
       "       1.75412265e-201, 4.56000639e-201, 4.75496015e-201, 8.93597679e-201,\n",
       "       3.07026507e-201, 7.62640986e-201, 6.90170735e-201, 1.25383175e-202,\n",
       "       6.58440564e-201, 9.98289431e-201, 2.49361967e-201, 3.97691163e-201,\n",
       "       2.13952447e-202, 4.14519077e-201, 2.40096536e-201, 3.74237655e-201,\n",
       "       4.40222390e-201, 5.62474474e-201, 8.73119481e-201, 5.93784525e-201,\n",
       "       4.47612889e-202, 3.02561356e-201, 6.51708833e-201, 6.41055534e-201,\n",
       "       5.75926352e-201, 9.17703655e-201, 6.65682849e-201, 8.44672344e-201,\n",
       "       5.45228263e-201, 1.84207198e-202, 8.08266142e-201, 6.53693845e-201,\n",
       "       6.61573201e-201, 7.55297799e-202, 2.60802225e-201, 6.11886755e-201,\n",
       "       7.17991345e-201, 6.53871504e-202, 3.37700525e-201, 7.23939003e-201,\n",
       "       8.90766773e-201, 1.62121518e-201, 1.82849531e-201, 2.36193064e-201,\n",
       "       7.88494367e-202, 5.77944904e-201, 3.91490302e-201, 9.86636731e-201,\n",
       "       6.55860178e-201, 6.18569349e-201, 7.11080171e-201, 6.56783583e-201,\n",
       "       7.98157019e-201, 9.37095422e-201, 7.26736309e-201, 1.01733643e-201,\n",
       "       7.43285696e-201, 4.22303506e-201, 8.22120073e-202, 2.60171009e-201,\n",
       "       4.53618985e-201, 4.57721842e-201, 2.55526309e-201, 4.61061936e-201,\n",
       "       9.91522747e-201, 8.50168570e-201, 4.50516770e-201, 6.49109238e-202,\n",
       "       8.00051570e-201, 8.84736801e-202, 4.56293774e-201, 8.86840829e-201,\n",
       "       9.92233021e-201, 2.35231283e-201, 8.05492637e-201, 1.59706768e-201,\n",
       "       4.88602803e-201, 2.50785636e-201, 9.31436747e-201, 1.85089539e-203,\n",
       "       7.08023777e-201, 1.08773185e-201, 8.86589482e-202, 1.39807423e-201,\n",
       "       2.01526292e-201, 3.21943775e-201, 1.77128214e-201, 9.42337231e-201,\n",
       "       7.31327482e-201, 7.99134189e-201, 5.46548019e-201, 9.36236194e-201,\n",
       "       7.09594160e-201, 7.64298571e-201, 9.01759205e-201, 3.08525197e-201,\n",
       "       8.38042977e-201, 2.28322887e-201, 5.44978664e-201, 6.33448641e-202,\n",
       "       8.98024306e-201, 8.43071443e-201, 5.47726255e-201, 8.63000861e-201,\n",
       "       9.51018735e-201, 2.56276953e-202, 5.26841743e-201, 3.60028314e-201,\n",
       "       8.11725172e-201, 9.40292690e-201, 3.99875774e-201, 4.38421799e-201,\n",
       "       6.51832804e-201, 8.54392329e-202, 5.77239666e-201, 2.47186343e-201,\n",
       "       5.60680417e-201, 4.65566322e-201, 1.17871172e-201, 2.88298072e-202,\n",
       "       9.10546124e-201, 7.77937812e-201, 1.48818704e-201, 5.66345643e-201,\n",
       "       8.69604966e-201, 2.04430475e-202, 2.01727410e-201, 6.00188604e-201,\n",
       "       1.63979914e-201, 8.79394356e-201, 5.87793343e-201, 8.21366836e-201,\n",
       "       7.34941934e-201, 1.55253687e-201, 7.87019668e-201, 6.29568695e-201,\n",
       "       4.16697910e-201, 8.33254362e-201, 7.42713369e-201, 9.63763462e-201,\n",
       "       5.01588041e-201, 3.46921707e-201, 4.34876641e-202, 5.00587357e-201,\n",
       "       9.05841728e-201, 8.81438297e-201, 3.37312539e-201, 2.23410161e-201,\n",
       "       4.59221257e-203, 9.91931430e-202, 4.80800563e-201, 3.25572361e-201,\n",
       "       9.60341394e-201, 1.58222546e-201, 9.84180720e-201, 2.37356633e-201,\n",
       "       6.22883850e-202, 3.78490559e-201, 8.63335693e-201, 6.68197649e-201,\n",
       "       8.50984114e-201, 8.55795154e-201, 5.29397618e-201, 7.56304815e-201,\n",
       "       2.32026492e-201, 1.38167548e-201, 6.13018982e-201, 9.61767281e-201,\n",
       "       3.32729791e-201, 3.01316415e-201, 7.58786276e-201, 1.04703107e-201,\n",
       "       6.78668386e-201, 8.69406622e-201, 1.47171081e-201, 3.79423613e-201,\n",
       "       3.10596597e-202, 3.10994526e-201, 9.69691899e-201, 4.30060703e-202,\n",
       "       1.74949124e-201, 4.19789479e-201, 9.16607106e-201, 5.84761948e-201,\n",
       "       6.76969433e-201, 7.03313423e-201, 2.97575826e-201, 7.63397893e-201,\n",
       "       7.36941647e-201, 7.59271442e-201, 1.02710159e-202, 3.89022682e-201,\n",
       "       1.45506338e-201, 6.27299661e-201, 2.81901781e-201, 4.90935528e-201,\n",
       "       1.93195494e-201, 1.40423920e-201, 5.76690511e-201, 5.38374903e-201,\n",
       "       6.83261805e-201, 7.08051593e-201, 9.45542290e-202, 3.76234961e-201,\n",
       "       6.11978275e-201, 7.01981281e-201, 9.38657751e-201, 4.01925088e-201,\n",
       "       7.70886457e-202, 3.26386604e-201, 8.35037224e-201, 6.37439689e-201])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.random.rand(1000) * 1e-200\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.047549521337438e-198"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_A = cuda.to_device(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.047549521337438e-198"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_reduce(d_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.2 µs ± 44.8 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit A.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.4 ms ± 860 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_reduce(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67 ms ± 6.08 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum_reduce(d_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.999999999999997e-17"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only support 1D array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-aa7cf8bf1f88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msum_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numba/cuda/kernels/reduction.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, arr, size, res, init, stream)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# ensure 1d array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"only support 1D array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;31m# adjust array size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only support 1D array"
     ]
    }
   ],
   "source": [
    "sum_reduce(d_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3276801280000.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_reduce(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
